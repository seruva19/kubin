{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Model\n",
    "\n",
    "model = '2.2-diffusers' #@param [\"2.0\", \"2.1\", \"2.1-diffusers\", \"2.2-diffusers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Settings\n",
    "\n",
    "branch = 'main' #@param [\"main\", \"dev\"]\n",
    "use_ngrok = False #@param {type:\"boolean\"}\n",
    "ngrok_auth_key = \"\" #@param {type:\"string\"}\n",
    "use_localtunnel = False #@param {type:\"boolean\"}\n",
    "use_cloudflared = False #@param {type:\"boolean\"}\n",
    "use_flash_attention = False #@param {type:\"boolean\"}\n",
    "save_models_to_gdrive = False #@param {type:\"boolean\"}\n",
    "save_images_to_gdrive = False #@param {type:\"boolean\"}\n",
    "\n",
    "use_diffusers = False\n",
    "\n",
    "if model == \"2.0\":\n",
    "  model_name = \"kd20\"\n",
    "\n",
    "if model == \"2.1\":\n",
    "  model_name = \"kd21\"\n",
    "\n",
    "if model == \"2.1-diffusers\":\n",
    "  model_name = \"kd21\"\n",
    "  use_diffusers = True\n",
    "\n",
    "if model == \"2.2-diffusers\":\n",
    "  model_name = \"kd22\"\n",
    "  use_diffusers = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Diffusers\n",
    "optimizations = []\n",
    "\n",
    "half_weights = True #@param {type:\"boolean\"}\n",
    "use_xformers = False #@param {type:\"boolean\"}\n",
    "use_sliced_attention = False #@param {type:\"boolean\"}\n",
    "use_sequential_offload = False #@param {type:\"boolean\"}\n",
    "channels_last_memory = False #@param {type:\"boolean\"}\n",
    "\n",
    "if half_weights: optimizations.append('half_weights')\n",
    "if use_xformers: optimizations.append('xformers')\n",
    "if use_sliced_attention: optimizations.append('sliced_attention')\n",
    "if use_sequential_offload: optimizations.append('sequential_offload')\n",
    "if channels_last_memory: optimizations.append('channels_last')\n",
    "\n",
    "optimizations = str.join(',', optimizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Extensions\n",
    "disabled_extensions = []\n",
    "\n",
    "image_browser = True #@param {type:\"boolean\"}\n",
    "interrogator = False #@param {type:\"boolean\"}\n",
    "mesh_gen = False #@param {type:\"boolean\"}\n",
    "prompt_styles = True #@param {type:\"boolean\"}\n",
    "segmentation = False #@param {type:\"boolean\"}\n",
    "upscaler = True #@param {type:\"boolean\"}\n",
    "training = False #@param {type:\"boolean\"}\n",
    "\n",
    "if not image_browser: disabled_extensions.append('kd-image-browser')\n",
    "if not interrogator: disabled_extensions.append('kd-interrogator')\n",
    "if not mesh_gen: disabled_extensions.append('kd-mesh-gen')\n",
    "if not prompt_styles: disabled_extensions.append('kd-prompt-styles')\n",
    "if not segmentation: disabled_extensions.append('kd-segmentation')\n",
    "if not upscaler: disabled_extensions.append('kd-upscaler')\n",
    "if not training: disabled_extensions.append('kd-training')\n",
    "\n",
    "disabled_extensions = str.join(',', disabled_extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title UI\n",
    "\n",
    "side_tabs_view = True #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_models_to_gdrive or save_images_to_gdrive:\n",
    "  from google.colab import drive\n",
    "\n",
    "  drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "\n",
    "[gpu] = !nvidia-smi --query-gpu=gpu_name --format=csv,noheader\n",
    "print (gpu)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "\n",
    "!git clone -b {branch} https://github.com/seruva19/kubin.git\n",
    "%cd /content/kubin\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "if use_diffusers:\n",
    "  !pip install -r diffusers/requirements.txt\n",
    "\n",
    "if use_flash_attention:\n",
    "  !pip install https://github.com/seruva19/flash-attn-wheels/raw/main/torch2.0.0%2Bcu118/flash_attn-1.0.1-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "if use_xformers:\n",
    "  !pip install xformers==0.0.20\n",
    "  !python -m xformers.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ngrok:\n",
    "  !pip install pyngrok\n",
    "  from pyngrok import ngrok\n",
    "  ngrok.set_auth_token(ngrok_auth_key)\n",
    "\n",
    "  tunnels = ngrok.get_tunnels()\n",
    "  for tunnel in tunnels:\n",
    "    ngrok.disconnect(tunnel.public_url)\n",
    "\n",
    "  print(ngrok.connect(7860, \"http\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_localtunnel:\n",
    "    !npm install -g localtunnel\n",
    "    lturl = !curl ipv4.icanhazip.com\n",
    "    print(\"endpoint IP for localtunnel:\")\n",
    "    print(lturl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cloudflared:\n",
    "    !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && dpkg -i cloudflared-linux-amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 $(lsof -t -i tcp:7860)\n",
    "\n",
    "!python src/kubin.py \\\n",
    "  --model-name={model_name} \\\n",
    "  --cache-dir={\"/content/gdrive/MyDrive/kubin/models\" if save_models_to_gdrive else \"/content/kubin/models\"} \\\n",
    "  --output-dir={\"/content/gdrive/MyDrive/kubin/output\" if save_images_to_gdrive else \"/content/kubin/output\"} \\\n",
    "  --disabled-extensions={disabled_extensions} \\\n",
    "  --pipeline={\"diffusers\" if use_diffusers else \"native\"} \\\n",
    "  --optimize={optimizations} \\\n",
    "  {\"--side-tabs='use'\" if side_tabs_view else \"\"} \\\n",
    "  --share={\"none\" if use_ngrok or use_localtunnel or use_cloudflared else \"gradio\"} \\\n",
    "  {\"--flash-attention='use'\" if use_flash_attention else \":\"} & \\\n",
    "  {\"npx localtunnel --port 7860\" if use_localtunnel else \":\"} & \\\n",
    "  {\"cloudflared tunnel --url http://localhost:7860\" if use_cloudflared else \":\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
